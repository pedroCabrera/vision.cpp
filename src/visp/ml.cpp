#include "visp/ml.h"
#include "util/string.h"
#include "visp/platform.h"

#include <algorithm>
#include <array>
#include <thread>
#include <vector>

#if defined(VISP_CUDA)
#include <ggml-cuda.h>
#endif

#if defined(VISP_VULKAN)
#include <ggml-vulkan.h>
#endif

namespace visp {

//
// backend

namespace {

// Global runtime overrides set by host apps (default to "no override").
// For booleans, -1 means use default behavior; 0 = false; 1 = true.
static int g_cuda_graphs_enabled = -1;       // -1 = default, 0 = disabled, 1 = enabled
static int g_cuda_allow_f16_interp = 0;      // default: disabled
static int g_flash_attention_enabled = -1;   // -1 = default per-backend, 0 = off, 1 = on

void disable_cuda_graphs_if_needed() {
#if defined(VISP_CUDA)
    static bool configured = false;
    if (configured) {
        return;
    }
    // Decide whether to enable or disable graphs based on runtime override or defaults
    int enable_graphs = g_cuda_graphs_enabled;
    if (enable_graphs == -1) {
        // No override set by application; choose build-time default
        // If build opted into CUDA graphs, leave enabled by default; else disable by default
        #if defined(VISP_CUDA_GRAPHS)
            enable_graphs = 1;
        #else
            enable_graphs = 0;
        #endif
    }
    // ggml-cuda controls this via GGML_CUDA_DISABLE_GRAPHS env; set it programmatically
    #if defined(_WIN32)
        _putenv_s("GGML_CUDA_DISABLE_GRAPHS", enable_graphs ? "0" : "1");
    #else
        setenv("GGML_CUDA_DISABLE_GRAPHS", enable_graphs ? "0" : "1", 1);
    #endif
    configured = true;
#endif
}

} // namespace

std::string_view to_string(backend_type type) {
    switch (type) {
        case backend_type::cpu: return "cpu";
        case backend_type::gpu: return "gpu";
        case backend_type::vulkan: return "vulkan";
        case backend_type::cuda: return "cuda";
        default: return "unknown";
    }
}

bool load_ggml_backends() {
    static const bool loaded = []() {
        #if defined(VISP_CUDA)
            disable_cuda_graphs_if_needed();
        #endif
        if (ggml_backend_reg_count() > 0) {
            return true; // already loaded
        }
        ggml_backend_load_all();
        if (ggml_backend_reg_count() == 0) {
            if (path dir = current_library_path(); !dir.empty()) {
                auto str = dir.parent_path().u8string();
                ggml_backend_load_all_from_path((char const*)str.c_str());
            }
        }
        return true;
    }();
    return loaded;
}

bool backend_is_available(backend_type type) {
    load_ggml_backends();
    switch (type) {
        case backend_type::cpu:
            return ggml_backend_dev_by_type(GGML_BACKEND_DEVICE_TYPE_CPU) != nullptr;
        case backend_type::gpu:
            return ggml_backend_dev_by_type(GGML_BACKEND_DEVICE_TYPE_GPU) != nullptr ||
                ggml_backend_dev_by_type(GGML_BACKEND_DEVICE_TYPE_IGPU) != nullptr;
        case backend_type::vulkan: {
            ggml_backend_reg_t reg = ggml_backend_reg_by_name("Vulkan");
            return reg && ggml_backend_reg_dev_count(reg) > 0;
        }
        case backend_type::cuda: {
            ggml_backend_reg_t reg = ggml_backend_reg_by_name("CUDA");
            return reg && ggml_backend_reg_dev_count(reg) > 0;
        }
        default: ASSERT(false, "Invalid backend type");
    }
    return false;
}

backend_device backend_init() {
    load_ggml_backends();
    backend_device b;
    b.handle.reset(ggml_backend_init_best());
    b.device = ggml_backend_get_device(b.handle.get());
    ASSERT(b.handle, "Failed to initialize backend");
    return b;
}

backend_device backend_init(backend_type type) {
    load_ggml_backends();

    backend_device b;
    switch (type) {
        case backend_type::cpu:
            b.handle.reset(ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr));
            break;
        case backend_type::gpu:
            b.handle.reset(ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_GPU, nullptr));
            if (!b.handle) {
                b.handle.reset(ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_IGPU, nullptr));
            }
            break;
        case backend_type::vulkan: {
            ggml_backend_reg_t vk_reg = ggml_backend_reg_by_name("Vulkan");
            if (vk_reg) {
                if (ggml_backend_reg_dev_count(vk_reg) > 0) {
                    ggml_backend_dev_t vk_dev = ggml_backend_reg_dev_get(vk_reg, 0);
                    if (vk_dev) {
                        b.handle.reset(ggml_backend_dev_init(vk_dev, nullptr));
                    }
                }
            }
            break;
        }
        case backend_type::cuda: {
            ggml_backend_reg_t cuda_reg = ggml_backend_reg_by_name("CUDA");
            if (cuda_reg) {
                if (ggml_backend_reg_dev_count(cuda_reg) > 0) {
                    ggml_backend_dev_t cuda_dev = ggml_backend_reg_dev_get(cuda_reg, 0);
                    if (cuda_dev) {
                        b.handle.reset(ggml_backend_dev_init(cuda_dev, nullptr));
                    }
                }
            }
            break;
        }
        default: ASSERT(false, "Invalid backend type");
    }
    if (!b.handle) {
        throw except("Failed to initialize backend, no suitable device available");
    }
    b.device = ggml_backend_get_device(b.handle.get());

    int nthreads = std::max(1, (int)std::thread::hardware_concurrency() - 2);
    backend_set_n_threads(b, nthreads);
    return b;
}

backend_type backend_device::type() const {
    ggml_backend_dev_t dev = ggml_backend_get_device(handle.get());
    switch (ggml_backend_dev_type(dev)) {
        case GGML_BACKEND_DEVICE_TYPE_CPU: return backend_type::cpu;
        case GGML_BACKEND_DEVICE_TYPE_GPU:
        case GGML_BACKEND_DEVICE_TYPE_IGPU: {
            std::string_view dev_name = ggml_backend_dev_name(dev);
            if (dev_name.find("CUDA") != std::string_view::npos) {
                return backend_type::cuda;
            }
            if (dev_name.find("Vulkan") != std::string_view::npos) {
                return backend_type::vulkan;
            }
            return backend_type::gpu;
        }
        default: ASSERT(false, "Unsupported backend device type"); return backend_type::cpu;
    }
}

typedef bool (*ggml_backend_dev_supports_f16_t)(ggml_backend_dev_t);

ggml_type backend_device::preferred_float_type() const {
    if (type() & backend_type::cpu) {
        return GGML_TYPE_F32; // not all operations support F16
    } else {
        ggml_backend_reg_t reg = ggml_backend_dev_backend_reg(device);
        if (void* f = ggml_backend_reg_get_proc_address(reg, "ggml_backend_dev_supports_f16")) {
            bool supports_f16 = ((ggml_backend_dev_supports_f16_t)f)(device);
            if (!supports_f16) {
                return GGML_TYPE_F32;
            }
        }
    }
    return GGML_TYPE_COUNT; // no preference, use float type of model weights
}

tensor_data_layout backend_device::preferred_layout() const {
    if (type() & backend_type::cpu) {
        return tensor_data_layout::cwhn;
    }
    if (type() == backend_type::cuda) {
        return tensor_data_layout::whcn;
    }
    return tensor_data_layout::unknown; // no preference, keep model weight layout
}

size_t backend_device::total_memory() const {
    ggml_backend_dev_t dev = ggml_backend_get_device(handle.get());
    size_t free, total;
    ggml_backend_dev_memory(dev, &free, &total);
    return total;
}

size_t backend_device::max_alloc() const {
    const size_t vulkan_max = 4 * 1024 * 1024 * 1024ULL; // TODO: query from backend
    const size_t cuda_max = 8 * 1024 * 1024 * 1024ULL;   // TODO: query from backend
    switch (type()) {
        case backend_type::vulkan: return vulkan_max;
        case backend_type::cuda: return cuda_max;
        default: return SIZE_MAX;
    }
}

void backend_set_n_threads(backend_device& b, int n_threads) {
    if (b.type() != backend_type::cpu) {
        return;
    }
    ggml_backend_reg_t reg = ggml_backend_dev_backend_reg(b.device);
    ggml_backend_set_n_threads_t set_n_threads =
        (ggml_backend_set_n_threads_t)ggml_backend_reg_get_proc_address(
            reg, "ggml_backend_set_n_threads");
    ASSERT(set_n_threads, "Failed to get backend set_n_threads function");
    set_n_threads(b.handle.get(), n_threads);
}

//
// model_build_flags

model_build_flags flash_attn_flag(bool default_enabled) {
    if (g_flash_attention_enabled == 1) {
        return model_build_flag::flash_attention;
    } else if (g_flash_attention_enabled == 0) {
        return model_build_flags{};
    }
    return default_enabled ? model_build_flag::flash_attention : model_build_flags{};
}

model_build_flags backend_default_flags(backend_type type) {
    using enum model_build_flag;
    switch (type) {
        case backend_type::cpu:
            return conv_2d_direct_cwhn | concat_n | f16_conv_transpose | window_partition |
                flash_attn_flag(false);
        case backend_type::gpu:
        case backend_type::vulkan: return flash_attn_flag(true);
        case backend_type::cuda: return flash_attn_flag(false);
    }
    return {};
}

model_build_flags model_get_build_flags(model_file const& file) {
    fixed_string<64> str;
    std::string_view arch = file.arch();
    model_build_flags flags;

    int64_t key = gguf_find_key(file.gguf.get(), format(str, "{}.tensor_data_layout", arch));
    if (key != -1) {
        std::string_view layout = gguf_get_val_str(file.gguf.get(), key);
        if (layout == "cwhn") {
            flags |= model_build_flag::cwhn;
        }
    }
    return flags;
}

//
// model_file

model_file model_load(char const* filepath) {
    ggml_context* data_ctx;
    gguf_init_params params;
    params.no_alloc = false;
    params.ctx = &data_ctx;

    gguf_context_ptr gguf_ctx(gguf_init_from_file(filepath, params));
    if (!gguf_ctx) {
        throw except("Failed to load GGUF model: {}", filepath);
    }
    return model_file{std::move(gguf_ctx), ggml_context_ptr(data_ctx), filepath};
}

int64_t model_file::n_tensors() const {
    return gguf_get_n_tensors(gguf.get());
}

int64_t model_file::key(char const* name) const {
    int64_t key_id = gguf_find_key(gguf.get(), name);
    if (key_id == -1) {
        throw except("Can't find key '{}' in model file {}", name, path);
    }
    return key_id;
}

std::string_view model_file::get_string(char const* key_name) const {
    return gguf_get_val_str(gguf.get(), key(key_name));
}

int model_file::get_int(char const* key_name) const {
    return gguf_get_val_i32(gguf.get(), key(key_name));
}

void model_file::get_array(char const* key_name, span<int> out_values) const {
    int64_t key_id = key(key_name);
    if (gguf_get_arr_n(gguf.get(), key_id) != out_values.size()) {
        throw except("Array size mismatch for key '{}' in model file {}", key_name, path);
    }
    if (gguf_get_arr_type(gguf.get(), key_id) != GGUF_TYPE_INT32) {
        throw except(
            "Array type mismatch for key '{}' in model file {}, expected int32", key_name, path);
    }
    auto ptr = (int const*)gguf_get_arr_data(gguf.get(), key_id);
    std::copy(ptr, ptr + out_values.size(), out_values.data());
}

std::string_view model_file::arch() const {
    return get_string("general.architecture");
}

ggml_type model_file::float_type() const {
    if (int64_t key_id = gguf_find_key(gguf.get(), "general.file_type"); key_id != -1) {
        if (gguf_get_kv_type(gguf.get(), key_id) == GGUF_TYPE_UINT32) {
            return (ggml_type)gguf_get_val_u32(gguf.get(), key_id);
        }
    }
    return GGML_TYPE_COUNT;
}

tensor_data_layout model_file::tensor_layout() const {
    fixed_string<64> str;
    int64_t key = gguf_find_key(gguf.get(), format(str, "{}.tensor_data_layout", arch()));
    if (key != -1) {
        std::string_view layout = gguf_get_val_str(gguf.get(), key);
        if (layout == "cwhn") {
            return tensor_data_layout::cwhn;
        } else if (layout == "whcn") {
            return tensor_data_layout::whcn;
        }
    }
    return tensor_data_layout::unknown;
}

//
// model_weights

model_weights model_init(size_t size) {
    ggml_init_params params{};
    params.mem_size = size * ggml_tensor_overhead();
    params.no_alloc = true;
    ggml_context_ptr ctx(ggml_init(params));

    model_weights w{};
    w.context = std::move(ctx);
    w.buffer_type = backend_type::cpu;
    return w;
}

bool model_allocate(model_weights& m, backend_device const& b) {
    ggml_backend_buffer_ptr buffer(ggml_backend_alloc_ctx_tensors(m.context.get(), b.handle.get()));
    if (!buffer) {
        return false; // context contains nothing to allocate
    }
    m.buffer_type = b.type();
    m.extra_buffers.push_back(std::move(buffer));
    return true;
}

namespace {

bool is_float_type(ggml_type t) {
    return t != GGML_TYPE_I8 && t != GGML_TYPE_I16 && t != GGML_TYPE_I32 && t != GGML_TYPE_I64;
}

int64_t max_tensor_elements(ggml_context* ctx) {
    int64_t result = 0;
    for (ggml_tensor* t = ggml_get_first_tensor(ctx); t; t = ggml_get_next_tensor(ctx, t)) {
        result = std::max(result, ggml_nelements(t));
    }
    return result;
}

ggml_type detect_float_type(ggml_context* ctx) {
    for (ggml_tensor* t = ggml_get_first_tensor(ctx); t; t = ggml_get_next_tensor(ctx, t)) {
        if (is_float_type(t->type)) {
            return t->type;
        }
    }
    return GGML_TYPE_F32;
}

template <typename T>
void permute_whcn_to_cwhn(T* n, bool depthwise) {
    if (depthwise) { // wh1c -> c1wh
        T perm[] = {n[3], n[2], n[0], n[1]};
        std::copy(perm, perm + 4, n);
    } else {
        std::swap(n[0], n[2]); // -> chwn
        std::swap(n[1], n[2]); // -> cwhn
    }
}

template <typename T>
void permute_cwhn_to_whcn(T* n, bool depthwise) {
    if (depthwise) { // c1wh -> wh1c
        T perm[] = {n[2], n[3], n[1], n[0]};
        std::copy(perm, perm + 4, n);
    } else {
        std::swap(n[1], n[2]); // -> chw n -> c h wn
        std::swap(n[0], n[2]); // -> whcn
    }
}

struct tensor_converter {
    ggml_context* weights_ctx = nullptr;
    ggml_type src_type;
    ggml_type dst_type;
    ggml_backend_ptr backend;
    ggml_context_ptr ctx;
    ggml_cgraph* graph;
    ggml_gallocr_ptr gallocr;
    ggml_tensor convert_src{};
    ggml_tensor* convert_dst;
    int layout_conversion;
    bool enabled = false;

    tensor_converter(ggml_context* weights, ggml_type target_type, int layout_conv)
        : weights_ctx(weights), dst_type(target_type), layout_conversion(layout_conv) {

        src_type = detect_float_type(weights);
        if (dst_type == GGML_TYPE_COUNT) {
            dst_type = src_type;
        }

        bool type_conversion = is_float_type(src_type) && dst_type != GGML_TYPE_COUNT && dst_type != src_type;
        bool layout_change = layout_conversion != 0;
        enabled = type_conversion || layout_change;
        if (!enabled) {
            return;
        }

        initialize_graph();
    }

    void initialize_graph() {
        if (ctx) {
            return;
        }

        ggml_init_params ctx_params{
            .mem_size = ggml_tensor_overhead() + ggml_graph_overhead(),
            .mem_buffer = nullptr,
            .no_alloc = true};
        ctx.reset(ggml_init(ctx_params));

        size_t max_elem = max_tensor_elements(weights_ctx);
        graph = ggml_new_graph_custom(ctx.get(), 2, false);
        convert_src.type = src_type;
        convert_src.ne[0] = max_elem;
        convert_src.nb[0] = ggml_type_size(src_type);
        for (int i = 1; i < GGML_MAX_DIMS; ++i) {
            convert_src.ne[i] = 1;
            convert_src.nb[i] = convert_src.nb[i - 1] * convert_src.ne[i - 1];
        }
        convert_dst = ggml_cast(ctx.get(), &convert_src, dst_type);
        ggml_set_output(convert_dst);
        ggml_build_forward_expand(graph, convert_dst);

        gallocr.reset(ggml_gallocr_new(ggml_backend_cpu_buffer_type()));
        ggml_gallocr_reserve(gallocr.get(), graph);

        backend.reset(ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr));
    }

    ggml_type target_type(ggml_tensor const* t) const {
        if (dst_type == GGML_TYPE_COUNT || !is_float_type(t->type)) {
            return t->type;
        }
        return dst_type;
    }

    void const* operator()(ggml_tensor const* src, ggml_tensor const* dst, bool convert_layout) {
        ggml_type dst_tensor_type = target_type(dst);
        bool need_type_conv = is_float_type(src->type) && src->type != dst_tensor_type;
        bool need_layout_conv = convert_layout && layout_conversion != 0;
        if (dst_type == GGML_TYPE_COUNT && !need_layout_conv) {
            return src->data;
        }
        if (!need_type_conv && !need_layout_conv) {
            return src->data;
        }
        if (!ctx) {
            initialize_graph();
        }
        ASSERT(ctx, "Weights contain tensors that would require conversion");

        convert_src.type = src->type;
        convert_src.data = src->data;
        std::copy(src->ne, src->ne + GGML_MAX_DIMS, convert_src.ne);
        std::copy(src->nb, src->nb + GGML_MAX_DIMS, convert_src.nb);
        if (need_layout_conv) {
            bool depthwise = convert_src.ne[2] == 1;
            if (layout_conversion > 0) {
                permute_whcn_to_cwhn(convert_src.ne, depthwise);
                permute_whcn_to_cwhn(convert_src.nb, depthwise);
            } else if (layout_conversion < 0) {
                permute_cwhn_to_whcn(convert_src.ne, depthwise);
                permute_cwhn_to_whcn(convert_src.nb, depthwise);
            }
        }

    convert_dst->type = dst_tensor_type;
    std::copy(dst->ne, dst->ne + GGML_MAX_DIMS, convert_dst->ne);
    std::copy(dst->nb, dst->nb + GGML_MAX_DIMS, convert_dst->nb);

        bool alloc_ok = ggml_gallocr_alloc_graph(gallocr.get(), graph);
        ASSERT(alloc_ok);

        ggml_backend_graph_compute(backend.get(), graph);
        return convert_dst->data;
    }
};

span<int32_t const> find_conv2d_weight_indices(model_file const& f) {
    gguf_context* gguf = f.gguf.get();
    auto name = format<fixed_string<64>>("{}.conv2d_weights", f.arch());
    int64_t key = gguf_find_key(gguf, name.c_str());
    if (key != -1 && gguf_get_arr_type(gguf, key) == GGUF_TYPE_INT32) {
        size_t n = gguf_get_arr_n(gguf, key);
        int32_t const* a = reinterpret_cast<int32_t const*>(gguf_get_arr_data(gguf, key));
        return span(a, n);
    }
    return {};
}

} // namespace

void model_transfer(
    ggml_context* const& src_ctx,
    model_weights& weights,
    backend_device const& device,
    ggml_type float_type,
    tensor_data_layout src_layout,
    tensor_data_layout dst_layout,
    span<int32_t const> conv2d_weights) {

    ggml_context* dst_ctx = weights.context.get();
    bool to_cwhn = src_layout == tensor_data_layout::whcn && dst_layout == tensor_data_layout::cwhn;
    bool to_whcn = src_layout == tensor_data_layout::cwhn && dst_layout == tensor_data_layout::whcn;
    int layout_conv = to_cwhn ? 1 : (to_whcn ? -1 : 0);
    tensor_converter convert(src_ctx, float_type, layout_conv);

    tensor orig = ggml_get_first_tensor(src_ctx);
    for (int64_t i = 0, conv2d_idx = 0; orig;) {
        if (strncmp(orig->name, "GGUF", 4) == 0) {
            orig = ggml_get_next_tensor(src_ctx, orig); // skip "GGUF tensor data binary blob"
            continue; // (why is there no way to iterate over GGUF tensors directly?)
        }
        auto ne = nelements(orig);
        if (conv2d_idx < ssize(conv2d_weights) && conv2d_weights[conv2d_idx] == i) {
            if (layout_conv > 0) {
                permute_whcn_to_cwhn(ne.data(), ne[2] == 1);
            } else if (layout_conv < 0) {
                permute_cwhn_to_whcn(ne.data(), ne[2] == 1);
            }
            ++conv2d_idx;
        }
        tensor dup = ggml_new_tensor(dst_ctx, convert.target_type(orig), GGML_MAX_DIMS, ne.data());
        ggml_set_name(dup, ggml_get_name(orig));
        orig = ggml_get_next_tensor(src_ctx, orig);
        ++i;
    }

    ggml_backend_buffer_t buffer = ggml_backend_alloc_ctx_tensors(dst_ctx, device);
    weights.weights_buffer = ggml_backend_buffer_ptr(buffer);
    weights.buffer_type = device.type();
    if (to_cwhn) {
        weights.flags |= model_build_flag::cwhn;
    } else if (to_whcn) {
        weights.flags = weights.flags & ~model_build_flags(model_build_flag::cwhn);
    }

    tensor src = ggml_get_first_tensor(src_ctx);
    tensor dst = ggml_get_first_tensor(dst_ctx);
    for (int i = 0, conv2d_idx = 0; src && dst;) {
        if (strncmp(src->name, "GGUF", 4) == 0) {
            src = ggml_get_next_tensor(src_ctx, src);
            continue; // skip "GGUF tensor data binary blob"
        }
        bool is_2d = conv2d_idx < int(conv2d_weights.size()) && conv2d_weights[conv2d_idx] == i;
        if (is_2d) {
            ++conv2d_idx;
        }
    void const* data = convert(src, dst, is_2d && layout_conv != 0);
        ggml_backend_tensor_set(dst, data, 0, ggml_nbytes(dst));
        src = ggml_get_next_tensor(src_ctx, src);
        dst = ggml_get_next_tensor(dst_ctx, dst);
        ++i;
    }
}

void model_transfer(
    model_file const& file,
    model_weights& weights,
    backend_device const& device,
    ggml_type float_type,
    tensor_data_layout layout) {

    weights.flags = model_get_build_flags(file);
    model_transfer(
        file.data.get(), weights, device, float_type, file.tensor_layout(), layout,
        find_conv2d_weight_indices(file));
}

ggml_type model_weights::float_type() const {
    for (ggml_tensor* t = ggml_get_first_tensor(context.get()); t != nullptr;
         t = ggml_get_next_tensor(context.get(), t)) {
        if (is_float_type(t->type)) {
            return t->type; // return first float type found
        }
    }
    return GGML_TYPE_COUNT;
}

//
// compute_graph

compute_graph compute_graph_init(size_t size) {
    ggml_init_params graph_ctx_params{};
    graph_ctx_params.mem_size = size * ggml_tensor_overhead() + ggml_graph_overhead();
    graph_ctx_params.no_alloc = true;
    ggml_context* ctx = ggml_init(graph_ctx_params);
    ggml_context_ptr ctx_ptr(ctx);
    ggml_cgraph* graph = ggml_new_graph_custom(ctx, size, false);
    return compute_graph{std::move(ctx_ptr), graph, nullptr};
}

bool compute_graph_allocate(compute_graph& g, backend_device const& backend) {
    if (!g.allocr) {
        g.allocr.reset(ggml_gallocr_new(ggml_backend_get_default_buffer_type(backend)));
    }
    bool result = ggml_gallocr_alloc_graph(g.allocr.get(), g.graph);
    if (!result) {
        throw std::runtime_error("Failed to allocate buffer for graph");
    }
    return result;
}

void compute(compute_graph const& g, backend_device const& b) {
    ggml_backend_graph_compute(b, g.graph);
}

//
// model_ref

model_ref::model_ref(model_weights& m)
    : weights_context(m.context.get()),
      graph_context(m.context.get()),
      graph(nullptr),
      flags(m.flags | backend_default_flags(m.buffer_type)),
            backend(m.buffer_type) {}

model_ref::model_ref(model_weights& m, compute_graph& g)
    : weights_context(m.context.get()),
      graph_context(g.context.get()),
      graph(g.graph),
      flags(m.flags | backend_default_flags(m.buffer_type)),
            backend(m.buffer_type) {}

model_ref::model_ref(
    ggml_context* weights_context,
    ggml_context* graph_context,
    ggml_cgraph* graph,
    model_build_flags flags,
    tensor_name prefix,
    backend_type backend)
    : weights_context(weights_context),
      graph_context(graph_context ? graph_context : weights_context),
      graph(graph),
      flags(flags),
      prefix(prefix),
      backend(backend) {}

tensor model_ref::find(char const* name) const {
    auto full_name = tensor_name();
    if (prefix) {
        name = format(full_name, "{}.{}", prefix.c_str(), name);
    }
    return ggml_get_tensor(weights_context, name);
}

tensor model_ref::weights(char const* name) const {
    if (tensor result = find(name)) {
        return result;
    }
    throw except("tensor not found: {}.{}", prefix.view(), name);
}

model_ref model_ref::with_prefix(tensor_name new_prefix) const {
    return model_ref{weights_context, graph_context, graph, flags, new_prefix, backend};
}

template <typename Stringable>
model_ref chain_prefix(model_ref const& m, Stringable sub_module) {
    if (m.prefix) {
        return m.with_prefix(format<tensor_name>("{}.{}", m.prefix.view(), sub_module));
    } else {
        return m.with_prefix(format<tensor_name>("{}", sub_module));
    }
}

model_ref model_ref::operator[](char const* sub_module) const {
    return chain_prefix(*this, sub_module);
}
model_ref model_ref::operator[](tensor_name sub_module) const {
    return chain_prefix(*this, sub_module.view());
}
model_ref model_ref::operator[](int sub_module) const {
    return chain_prefix(*this, sub_module);
}

tensor named(model_ref const& m, tensor tensor) {
    ggml_set_name(tensor, m.prefix.c_str());
    return tensor;
}

//
// tensor creation and data handling

tensor compute_graph_input(model_ref const& m, ggml_type type, i64x4 shape, tensor_name name) {
    tensor x = ggml_new_tensor_4d(m, type, shape[0], shape[1], shape[2], shape[3]);
    ggml_set_name(x, name.c_str());
    ggml_set_input(x);
    return x;
}

tensor compute_graph_output(model_ref const& m, tensor x, tensor_name name) {
    ggml_set_name(x, name.c_str());
    ggml_set_output(x);
    ggml_build_forward_expand(m.graph, x);
    return x;
}

tensor_data tensor_alloc(tensor x) {
    return {x, std::unique_ptr<byte[]>(new byte[ggml_nbytes(x)])};
}

tensor_data tensor_load(tensor x, char const* filepath) {
    FILE* file = fopen(filepath, "rb");
    if (!file) {
        throw except("Failed to open file: {}", filepath);
    }
    tensor_data result = tensor_alloc(x);
    size_t read = fread(result.data.get(), 1, ggml_nbytes(x), file);
    fclose(file);
    if (read != ggml_nbytes(x)) {
        throw except("Failed to read data from file: {}", filepath);
    }
    return result;
}

void tensor_save(tensor x, char const* filepath) {
    FILE* file = fopen(filepath, "wb");
    if (!file) {
        throw except("Failed to open file for writing: {}", filepath);
    }
    size_t written = fwrite(x->data, 1, ggml_nbytes(x), file);
    fclose(file);
    if (written != ggml_nbytes(x)) {
        throw except("Failed to write tensor data to file: {}", filepath);
    }
}

std::span<float> tensor_data::as_f32() {
    ASSERT(x->type == GGML_TYPE_F32);
    return span(reinterpret_cast<float*>(data.get()), ggml_nelements(x));
}

std::span<float const> tensor_data::as_f32() const {
    ASSERT(x->type == GGML_TYPE_F32);
    return span(reinterpret_cast<float const*>(data.get()), ggml_nelements(x));
}

std::span<int32_t> tensor_data::as_i32() {
    ASSERT(x->type == GGML_TYPE_I32);
    return span(reinterpret_cast<int32_t*>(data.get()), ggml_nelements(x));
}

std::span<int32_t const> tensor_data::as_i32() const {
    ASSERT(x->type == GGML_TYPE_I32);
    return span(reinterpret_cast<int32_t const*>(data.get()), ggml_nelements(x));
}

std::span<byte> tensor_data::as_bytes() {
    return span(data.get(), ggml_nbytes(x));
}

std::span<byte const> tensor_data::as_bytes() const {
    return span(data.get(), ggml_nbytes(x));
}

void transfer_to_backend(tensor_data const& d) {
    ggml_backend_tensor_set(d.x, d.data.get(), 0, ggml_nbytes(d.x));
}

void transfer_to_backend(tensor x, std::span<const byte> data) {
    ASSERT(ggml_nbytes(x) == data.size_bytes());
    ggml_backend_tensor_set(x, data.data(), 0, ggml_nbytes(x));
}

void transfer_to_backend(tensor x, std::span<const float> data) {
    ASSERT(ggml_nbytes(x) == data.size_bytes());
    ggml_backend_tensor_set(x, data.data(), 0, ggml_nbytes(x));
}

void transfer_to_backend(tensor x, image_view const& img) {
    ASSERT(ggml_nbytes(x) == n_bytes(img));
    ggml_backend_tensor_set(x, img.data, 0, ggml_nbytes(x));
}

tensor_data transfer_from_backend(tensor x) {
    tensor_data result = tensor_alloc(x);
    ggml_backend_tensor_get(x, result.data.get(), 0, ggml_nbytes(x));
    return result;
}

void transfer_from_backend(tensor x, span<float> dst, size_t offset) {
    size_t size = std::min(dst.size_bytes(), ggml_nbytes(x) - offset);
    ggml_backend_tensor_get(x, dst.data(), offset, size);
}

void transfer_from_backend(tensor x, image_span const& dst) {
    ASSERT(ggml_nbytes(x) == n_bytes(dst));
    ggml_backend_tensor_get(x, dst.data, 0, ggml_nbytes(x));
}

//
// tensor operations

tensor slice(model_ref const& m, tensor x, slice_t s0, slice_t s1, slice_t s2, slice_t s3) {
    ASSERT(s0.step == 1 && "Slice step must be 1 for the begin dimension");

    auto ne = std::array{x->ne[0], x->ne[1], x->ne[2], x->ne[3]};
    auto nb = std::array{x->nb[0], x->nb[1], x->nb[2], x->nb[3]};
    auto slices = std::array{s0, s1, s2, s3};
    size_t offset = 0;

    for (int dim = 0; dim < 4; ++dim) {
        auto [begin, end, step] = slices[dim];
        end = end == slice_t::max ? x->ne[dim] : end;
        end = end < 0 ? x->ne[dim] + end : end;
        begin = begin < 0 ? x->ne[dim] + begin : begin;
        ASSERT(begin >= 0 && end <= x->ne[dim] && "Slice indices out of bounds");
        ASSERT(begin < end && "Begin index must be less than end index");

        ne[dim] = (end - begin + step - 1) / step;
        nb[dim] = x->nb[dim] * step;
        offset += begin * x->nb[dim];
    }
    return ggml_view_4d(m, x, ne[0], ne[1], ne[2], ne[3], nb[1], nb[2], nb[3], offset);
}

tensor concat(model_ref const& m, std::array<tensor, GGML_MAX_SRC> src, int dim) {
    int n = (int)std::count_if(src.begin(), src.end(), [](tensor t) { return t != nullptr; });

    // On CUDA, certain concat kernels currently require F32 inputs. If all inputs are
    // floating-point and share the same non-F32 dtype (e.g., F16), temporarily promote
    // to F32 for the concat and cast back to the original dtype to preserve model dtypes.
    bool needs_high_precision_concat = false;
    ggml_type original_type = GGML_TYPE_COUNT; // invalid sentinel
    if (m.backend == backend_type::cuda && n > 0) {
        original_type = src[0]->type;
        bool all_float = true;
        bool all_same_type = true;
        for (int i = 0; i < n; ++i) {
            if (!is_float_type(src[i]->type)) {
                all_float = false;
                break;
            }
            if (src[i]->type != original_type) {
                all_same_type = false;
            }
        }
        needs_high_precision_concat = all_float && all_same_type && original_type != GGML_TYPE_F32;
    }

    if (needs_high_precision_concat) {
        std::array<tensor, GGML_MAX_SRC> src_cast{};
        for (int i = 0, j = 0; i < (int)src.size(); ++i) {
            if (src[i] == nullptr) continue;
            src_cast[j++] = ggml_cast(m, src[i], GGML_TYPE_F32);
        }

        tensor x;
        if (m.flags & model_build_flag::concat_n) {
            x = ggml_concat_n(m, src_cast.data(), n, dim);
        } else {
            x = src_cast[0];
            for (int i = 1; i < n; ++i) {
                x = ggml_concat(m, x, src_cast[i], dim);
            }
        }
        return ggml_cast(m, x, original_type);
    }

    if (m.flags & model_build_flag::concat_n) {
        return ggml_concat_n(m, src.data(), n, dim);
    } else {
        tensor x = src[0];
        for (int i = 1; i < n; ++i) {
            x = ggml_concat(m, x, src[i], dim);
        }
        return x;
    }
}

tensor interpolate(model_ref const& m, tensor x, i64x2 target, int32_t mode) {
    // Promote to F32 for CUDA only; Vulkan stays in requested type unless kernels require otherwise
    // Allow override via visp_set_cuda_allow_f16_interpolation(true) to keep FP16 for bilinear/bicubic when safe
    bool use_high_precision =
        (m.backend == backend_type::cuda) && !g_cuda_allow_f16_interp &&
        is_float_type(x->type) && x->type != GGML_TYPE_F32 &&
        ((mode & GGML_SCALE_MODE_BILINEAR) || (mode & GGML_SCALE_MODE_BICUBIC));
    tensor original = x;
    if (use_high_precision) {
        x = ggml_cast(m, x, GGML_TYPE_F32);
    }

    tensor result;
    if ((m.flags & model_build_flag::cwhn) && mode == GGML_SCALE_MODE_NEAREST) {
        result = ggml_interpolate(m, x, x->ne[0], target[0], target[1], x->ne[3], mode);
    } else {
        // Bilinear interpolation requires WHCN layout!
        result = ggml_interpolate(m, x, target[0], target[1], x->ne[2], x->ne[3], mode);
    }

    if (use_high_precision) {
        result = ggml_cast(m, result, original->type);
    }
    return result;
}

} // namespace visp

// Runtime setters
namespace visp {

void visp_set_cuda_graphs_enabled(bool enable) {
    g_cuda_graphs_enabled = enable ? 1 : 0;
}

void visp_set_cuda_allow_f16_interpolation(bool allow) {
    g_cuda_allow_f16_interp = allow ? 1 : 0;
}

void visp_set_flash_attention_enabled(bool enable) {
    g_flash_attention_enabled = enable ? 1 : 0;
}

} // namespace visp
